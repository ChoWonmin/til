# Aggregation

|name||
|-|-|
|group by||
|window|함수의 입력으로 사용할 로우는 현재의 로우와 어느 정도 연관성이 필요|
|grouping set|서로 다른 레벨의 값을 집계|
|rollup|계층적으로 요약된 값|
|cube|모든 컬럼 조합에 대한 요약 값|

### 집계 함수

|funtion|args|desc|
|-|-|-|
|count|-|row 개수|
|countDistinct|column|고유한 row 개수|
|approx_count_distinct|column, maximumn estimation error|고유한 row 개수 근사치|
|sum|column|합계|
|sumDistinct|column|고유값 합계|
|stddev_pop|column|모표준편차|
|stddev_samp|column|표본표준편차|
|var_pop|column|모표준분산|
|var_samp|column|표본표준분산|
|skewness|column|비대칭도|
|kurtosis|column|첨도|
|corr|column, column|상관계수|
|covar_pop|column, column|모공분산|
|covar_samp|column, column|표본공분산|

### 그룹화

- 단일 컬럼의 데이터를 그룹화하고 해당 그룹의 다른 여러 컬럼을 사용해서 계산
- 카테고리형 데이터

#### example
``` python
df.groupBy("InvoiceNo", "CustomerId").count().show()

df.groupBy("InvoiceNo").agg(
  count("Quantity").alias("quan"),
  expr("count(Quantity)")).show()
```

### Window

- 특정 윈도우를 대상의로 집계

#### window function

1.Ranking functions
- rank
- denseRank
- percentRank
- ntile
- rowNumber

2.Analytic functions 
- cumeDist
- firstValue
- lastValue
- lag
- lead

#### window specification

1. Partitioning Specification :: **prtitionBy**
2. Ordering Specification :: **orderBy**
3. Frame Specification :: **rowsBetween**

---
## Reference

https://databricks.com/blog/2015/07/15/introducing-window-functions-in-spark-sql.html
